{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Keypoint Detection using YOLOv11\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook will guide you through using YOLOv11 (You Only Look Once) Algorithm to detect 21 different keypoints on human hand by training the YOLO model on a Ultralytics Hand Keypoints dataset which contains over 26,768 images.\n",
    "\n",
    "## Credits\n",
    "\n",
    "- Dataset: [Roboflow](https://docs.ultralytics.com/datasets/pose/hand-keypoints/)\n",
    "- Reference: [Roboflow](https://docs.ultralytics.com/datasets/pose/hand-keypoints/)\n",
    "- Model: [Ultralytics](https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Prerequisites](#prerequisites)\n",
    "- [Installing YOLOv11](#installing-yolov11)\n",
    "- [Getting the Dataset](#getting-the-dataset)\n",
    "- [Training](#training)\n",
    "- [Testing](#testing)\n",
    "- [Performance](#performance)  \n",
    "- [WebCam Testing](#webcam-testing)\n",
    "- [Usage](#usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "We'll be using following tools for this notebook\n",
    "- Ultralytics YOLOv11 Model\n",
    "- Roboflow Car parts Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have access to GPU for faster computation. Run `nvidia-smi` command and check if you get output something like following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the current working directory is root directory of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing YOLOv11\n",
    "\n",
    "Ultralytics package includes all necessary libraries and dependencies used to run YOLOv11. So installation is quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that ultralytics is installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.9 ðŸš€ Python-3.8.10 torch-2.0.0+cu117 CUDA:0 (Tesla V100-PCIE-32GB, 32510MiB)\n",
      "Setup complete âœ… (64 CPUs, 376.6 GB RAM, 120.9/136.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Dataset\n",
    "\n",
    "Dataset will be fetched using the predefined yaml file for hand keypoints dataset. The training will start after fetching the dataset.\n",
    "\n",
    "- Name: Ultralytics Hand Keypoints Dataset\n",
    "- Format: YOLOv11\n",
    "- Images count: 26,768\n",
    "- Image size: 640x640\n",
    "- Keypoints count: 21\n",
    "- Train/Val/Test Distribution: 70%:30%:0%\n",
    "\n",
    "## Training\n",
    "\n",
    "Run the following code to get the dataset and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"hand-keypoints/data.yaml\", epochs=10, imgsz=640, save=True, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Some images in the dataset might be corrupted or non-normalized. You can ignore the warnings related to them if you encounter any.\n",
    "\n",
    "The model was trained for only 10 epochs because of resource constraints, since long training times cause GPU overheating and throttling and cause the process to slow down.  You can increase the number of epochs for better results if you have more resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's test our model performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(f\"{HOME}/runs/pose/train/weights/best.pt\") \n",
    "\n",
    "metrics = model.val() # No need to explicitly pass arguments: imgsz, data, conf, batch, etc. model will use the values specified during training\n",
    "metrics.box.map  \n",
    "metrics.box.map50  \n",
    "metrics.box.map75 \n",
    "metrics.box.maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "YOLO provides a detailed visualisation of the model performance. Check out `runs/pose/train` folder for training performance, and `runs/pose/val` folder for validation performance.\n",
    "\n",
    "We achieved a mAP of 0.805 which is sufficiently reliable for any keypoint detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebCam Testing\n",
    "\n",
    "You can detect hand keypoints in realtime using OpenCV library for performing detections of Video Stream from Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    result = model.predict(frame)\n",
    "\n",
    "    cv2.imshow('Webcam Video Stream', result[0].plot())\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "YOLO weights are now upgraded to detect hand keypoints. Now you can use the trained weights for detecting hand gestures.\n",
    "\n",
    "The trained weights are stored in `runs/pose/train/weights/best.pt` file. To use the trained weights on you images, first load the weights in a YOLO model. Then, use the model to detect hand keypoints in your images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portable Weights\n",
    "\n",
    "Now you can use you trained model any any machine with ultralytics installed. Just copy the weights `runs/pose/train/weights/best.pt` to your machine and load the YOLO model with those weights.\n",
    "\n",
    "Your model is now ready to _read_ your gestures!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
